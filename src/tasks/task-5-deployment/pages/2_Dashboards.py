import streamlit as st
import webbrowser
st.set_page_config(layout="wide")


st.title("Dashboards")

st.write(
    "Explore interactive dashboards created by the EDA team, along with inferences from the data. "
    "Gain valuable insights into waste management trends and patterns."
)

# Creating tabs
tab1, tab2, tab3, tab4 = st.tabs(
    ['Waste Generated Nationally', 'Waste Generated by State', 'Hazardous Waste', 'Geospatial Data'])

with tab1:
    st.markdown(
        f'<h1 style="text-align: center;">Waste Generated Nationally</h1>',
        unsafe_allow_html=True
    )
    # import pygwalker as pyg
    # import pandas as pd
    # from gensim import corpora, models
    # import gensim
    # from sklearn.feature_extraction.text import CountVectorizer
    # import nltk
    # nltk.download('punkt')
    # from sumy.parsers.plaintext import PlaintextParser
    # from sumy.nlp.tokenizers import Tokenizer
    # from sumy.summarizers.lex_rank import LexRankSummarizer

    # df = pd.read_csv("data/waste_generated_32161-0001.csv", encoding='ISO-8859-1')
    # df.dropna(inplace = True)

    # vectorizer = CountVectorizer()
    # X = vectorizer.fit_transform(df['types_of_waste'].values)

    # corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)

    # id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())
    # lda = models.LdaModel(corpus, num_topics=30, id2word=id2word, passes=15)

    # df['Topic_Label'] = [max(lda.get_document_topics(item), key=lambda x: x[1])[0] for item in corpus]
    # df['Topic_Label'].unique()

    # num_topics = 30
    # for topic_id in range(num_topics):
    #     sample_waste_types = df[df['Topic_Label'] == topic_id]  
    #     ['types_of_waste'].sample(5)  
    #     print(f"Topic {topic_id}:")
    #     for waste_type in sample_waste_types:
    #         print(waste_type)
    #     print("\n")

    # summarizer = LexRankSummarizer()

    # category_names = {}

    # Iterate over each unique topic label
    # for topic_id in df['Topic_Label'].unique():
        # Get all waste types for the current topic
        # topic_text = ' '.join(df[df['Topic_Label'] == topic_id] ['types_of_waste'])

        # Create a plaintext parser
        # parser = PlaintextParser.from_string(topic_text,Tokenizer  ("english"))

        # Summarize the text to generate a category name
        # summary = summarizer(parser.document, sentences_count=1)  

        # Store the generated category name in the dictionary
        # category_names[topic_id] = str(summary[0])
    
    # df['Category_Name'] = df['Topic_Label'].map(category_names)

    # walker4 = pyg.walk(
        # df,
        # spec="data/gw0.json",       
        # use_kernel_calc=True,    
        # use_preview=True,        
    # )

    # data = df.loc[:, ['year', 'Topic_Label', 'generated_waste_quantity']]

    # df1 = pd.DataFrame(df)
    # topic_label_totals = df.groupby('Category_Name')['generated_waste_quantity'].sum().reset_index()

with tab2:
    st.markdown(
            f'<h1 style="text-align: center;">Waste Generated by State</h1>',
            unsafe_allow_html=True
        )
        # st.header("Waste Generated by State")
    
    import streamlit as st
    import pandas as pd
    import pygwalker as  pyg


    # Set page Configurations
    # st.set_page_config(
    #   page_title='PyGWalker Demo',
    #   page_icon=':snake:',
    #   layout='wide',
    #   initial_sidebar_state='expanded'
    # )

    # Load data
    @st.cache_data
    def load_data(url):
      df = pd.read_csv(url,sep=',',encoding='latin1')
      return df

    df = load_data('data/Amount-of Waste-Generated-By-State 32121-0003.csv',)

    # Dispay PygWalker 
    def load_config(file_path):
      with open(file_path,'r') as config_file:
        config_str = config_file.read()
      return config_str

    config = load_config('data/config.json')
    pyg.walk(df,env ='Streamlit',dark='dark',spec=config)

with tab3:
    st.markdown(
        f'<h1 style="text-align: center;">Hazardous Waste</h1>',
        unsafe_allow_html=True
    )
   
    import streamlit as st
    import plotly.express as px
    import pandas as pd
    import numpy as np
    import os
    import re
    import statsmodels
    from wordcloud import WordCloud
    import warnings
    import string
    warnings.filterwarnings('ignore')
    stopwords = ["waste", "and", "from", "other", "etc",    "still", "it", "it's", "its", "they", "this",  "that",     "that'll",
            "these", "those", "is", "are", "was", "were", "be", "been", "being", "have", "has",     "had", "having", "do",
            "does", "a", "an", "the", "and", "as", "while", "of", "at", "by", "for", "with", "into",    "to", "from",
            "in", "out", "on"]
    Most_Important_labels = 10
    def clean_text(text):
        text = text.lower().strip()
        text = ''.join([char for char in text if char not in string.punctuation])
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        tokens = text.split()
        tokens = [word for word in tokens if word not in stopwords]
        cleaned_text = ' '.join(tokens)
        return cleaned_text
    def main():
        st.markdown('<style>div.block-container{text-align: center}{border:1px solid red}{padding-top:0.5rem;}</style>',
                    unsafe_allow_html=True)
        overall_waste_qty = pd.read_csv('data/hazardous_EAV2-6_32151-0002.csv')
        statewise_waste_qty = pd.read_csv('data/hazardous_EAV2-6_32151-0003.csv')
        column1, column2 = st.columns((2))
        # Preprocessing on overall_waste_qty dataframe
        overall_waste_qty = overall_waste_qty.rename(
            columns={'year': 'year', 'type of waste': 'type_of_waste', 'waste producers': 'waste_producers',
                    'waste quantities': 'waste_quantities',
                    'waste quantities generated by primary producers': 'waste_quantities_generated_by_primary_producers'})
        overall_waste_qty['type_of_waste'] = overall_waste_qty['type_of_waste'].apply(clean_text)
        if 'Unnamed: 0' in overall_waste_qty.columns.tolist():
            overall_waste_qty.drop(['Unnamed: 0'], axis=1, inplace=True)
        overall_waste_qty = overall_waste_qty.astype(
            {'waste_producers': int, 'waste_quantities': float, 'waste_quantities_generated_by_primary_producers': float,
            'year': str, 'type_of_waste': str})
        statewise_waste_qty = statewise_waste_qty.rename(
            columns={'Year ': 'year', 'Federal State ': 'federal_state', 'Number of Waste Producers': 'waste_producers',
                    'Waste Quantity (1000 t)': 'waste_quantities',
                    'Waste Quantity Handed Over to Primary Producers (1000 t)': 'waste_quantities_returned_to_primary_producers'})
        statewise_waste_qty = statewise_waste_qty.astype({'year': str})
        statewise_waste_qty['waste_producers'] = pd.to_numeric(statewise_waste_qty['waste_producers'], errors='coerce')
        statewise_waste_qty['waste_quantities'] = pd.to_numeric(statewise_waste_qty['waste_quantities'], errors='coerce')
        statewise_waste_qty['waste_quantities_returned_to_primary_producers'] = pd.to_numeric(
            statewise_waste_qty['waste_quantities_returned_to_primary_producers'], errors='coerce')
        statewise_waste_qty['waste_qty_per_producer'] = statewise_waste_qty['waste_quantities'] / statewise_waste_qty[
            'waste_producers']
        statewise_waste_qty.loc[statewise_waste_qty['waste_qty_per_producer'] == np.nan, 'waste_qty_per_producer'] = -1
        startYear = overall_waste_qty['year'].min()
        endYear = overall_waste_qty['year'].max()
        with column1:
            start_year_input = st.text_input("Start Year", startYear)
            end_year_input = st.text_input("End Year", endYear)
            overall_waste_qty1 = overall_waste_qty.loc[
                (overall_waste_qty['year'] >= start_year_input) & (overall_waste_qty['year'] <= end_year_input)].copy(deep=True)
            statewise_waste_qty1 = statewise_waste_qty.loc[
                (statewise_waste_qty['year'] >= start_year_input) & (statewise_waste_qty['year'] <= end_year_input)].copy(
                deep=True)
            st.subheader(f"Distribution of yearly waste quantity produced (1000 tons) | {start_year_input} - {end_year_input}")
            fig = px.histogram(overall_waste_qty1, x='waste_quantities', template='seaborn', histnorm='probability')
            st.plotly_chart(fig, use_container_width=True, height=200)
            st.text('This is some interpretation of Figure 1.')
            st.subheader(f"Trend of Total Waste Quantity produced | {start_year_input} - {end_year_input}")
            overall_waste_qty1_grouped = overall_waste_qty1.groupby(by=["year"]).sum()[
                ["waste_quantities", "waste_quantities_generated_by_primary_producers"]].reset_index()
            fig = px.line(overall_waste_qty1_grouped, x="year",
                        y=["waste_quantities", "waste_quantities_generated_by_primary_producers"], template='seaborn')
            st.plotly_chart(fig, use_container_width=True, height=200)
            st.text("This is some interpretation of Figure 3")
            st.subheader(f"Waste Quantity produced v/s Number of Producers | {start_year_input} - {end_year_input}")
            fig = px.scatter(overall_waste_qty1, x="waste_producers",
                            y=["waste_quantities", "waste_quantities_generated_by_primary_producers"], template='seaborn',
                            trendline='ols')
            st.plotly_chart(fig, use_container_width=True, height=200)
            st.text("This is some interpretation of Figure 5")
            text = ' '.join(overall_waste_qty1['type_of_waste'])
            wordcloud = WordCloud(width=1000, height=400, background_color='white').generate(text)
            st.image(wordcloud.to_array(), use_column_width=True)
            st.text("These are the most common words in the description")
        with column2:
            states = st.multiselect("Select the states for analysis", statewise_waste_qty1['federal_state'].unique())
            if not states:
                statewise_waste_qty2 = statewise_waste_qty1.copy(deep=True)
            else:
                statewise_waste_qty2 = statewise_waste_qty1.loc[statewise_waste_qty1['federal_state'].isin(states)]
            st.subheader(f"Distribution of yearly waste quantity produced (1000 tons) | Statewise | {start_year_input} - {end_year_input}   ")       
            fig = px.histogram(statewise_waste_qty2, y='waste_quantities', x='federal_state', template='seaborn')
            st.plotly_chart(fig, use_container_width=True, height=200)
            st.text('This is some interpretation of Figure 2.')

            st.subheader(f"Trend of Waste Quantity produced | Statewise | {start_year_input} - {end_year_input}")
            all_selected_states = statewise_waste_qty2['federal_state'].unique().tolist()
            statewise_waste_qty2_grouped = pd.DataFrame()
            statewise_waste_qty2_grouped['year'] = sorted(statewise_waste_qty2['year'].unique().tolist())
            columns_to_plot = []
            for each_state in all_selected_states:
                each_state_waste_qty = \
                    statewise_waste_qty2.loc[statewise_waste_qty2['federal_state'] == each_state].groupby(by=["year"]).sum()[
                        ["waste_quantities"]].reset_index().sort_values(by=['year'])
                statewise_waste_qty2_grouped[each_state] = each_state_waste_qty['waste_quantities']
                columns_to_plot.append(each_state)
            fig = px.line(statewise_waste_qty2_grouped, x="year", y=columns_to_plot, template='seaborn')
            st.plotly_chart(fig, use_container_width=True, height=200)
            st.text("This is some interpretation of Figure 4")
            statewise_waste_qty2_grouped = pd.DataFrame()
            statewise_waste_qty2_grouped['year'] = sorted(statewise_waste_qty2['year'].unique().tolist())
            columns_to_plot = []
            for each_state in all_selected_states:
                each_state_waste_qty = \
                    statewise_waste_qty2.loc[statewise_waste_qty2['federal_state'] == each_state].groupby(by=["year"]).sum()[
                        ["waste_qty_per_producer"]].reset_index().sort_values(by=['year'])
                statewise_waste_qty2_grouped[each_state] = each_state_waste_qty['waste_qty_per_producer']
                columns_to_plot.append(each_state)
            st.subheader(f"Trend of Waste Quantity produced per producer | {start_year_input} - {end_year_input}")
            fig = px.line(statewise_waste_qty2_grouped, x="year", y=columns_to_plot, template='seaborn')
            st.plotly_chart(fig, use_container_width=True, height=200)
            st.text("This is some interpretation of Figure 6")
    if __name__ == '__main__':
        main()


with tab4:
    # from data import GeoApp
    # GeoApp.main()
    import streamlit as st
    import pandas as pd
    import plotly.express as px
    import geopandas as gpd
    import matplotlib.pyplot as plt
    import plotly.graph_objects as go

    # st.set_page_config(layout="wide")

    #main Dataset
    def load_data():
        data = pd.read_csv("data/GeoData.csv")
        return data
    data = load_data()


    st.markdown(
        f'<h1 style="text-align: center;">Germany<br>Waste-Management Stations<br> GeoSpatial Analysis</h1>',
        unsafe_allow_html=True
    )

    # DataFrame - station counts by state and type
    station_counts = data.groupby(['state', 'station']).size().unstack(fill_value=0)

    # Total stations for each state
    station_counts['Total Stations'] = station_counts.sum(axis=1)

    # Totals for each station type
    station_type_totals = station_counts.sum(axis=0)
    station_type_totals.name = 'Total'
    station_counts = pd.concat([station_counts, station_type_totals.to_frame().T])

    # Station Information Table
    st.markdown(
        f'<h2 style="color: Brown"><br><br>Station Information Table</h2>',
        unsafe_allow_html=True
    )

    # dropdown for states
    selected_states = st.multiselect("Select States", data['state'].unique(), default=data['state'].unique())

    # Filter states
    filtered_data = data[data['state'].isin(selected_states)]

    # table with state-wise station counts
    st.table(station_counts[station_counts.index.isin(selected_states)].reset_index())

    # station counts by state and type
    station_counts_by_type = filtered_data.groupby(['state', 'station']).size().unstack(fill_value=0)

    # total stations for each state
    station_counts_by_type['Total Stations'] = station_counts_by_type.sum(axis=1)



    # Station Information Grid
    st.header(" ")
    st.header(" :blue[Station Information Grid]")
    station_info = [
        {
            'name': 'Landfills',
            'type': 'landfills',
            'color': '#ff4c67'
        },
        {
            'name': 'Waste Disposal Centres',
            'type': 'waste disposal centres',
            'color': '#ef9dff'
        },
        {
            'name': 'Recycling Centres',
            'type': 'recycling centres',
            'color': '#26A288'
        },
        {
            'name': 'Waste Transfer Stations',
            'type': 'waste transfer stations',
            'color': '#86983e'
        }
    ]

    col1, col2 = st.columns(2)

    for info in station_info:
        station_name = info['name']
        station_type = info['type']
        filtered_data_by_type = filtered_data[filtered_data['station'] == station_type]
        station_count = len(filtered_data_by_type)

        # Display station information in a grid cell with styles and spacing
        with col1 if info['name'] in ('Landfills', 'Waste Disposal Centres') else col2:
            st.markdown(
                f'<div class="grid-item" style="background-color: {info["color"]}; padding: 20px; '
                f'border-radius: 5px; color: black; margin-bottom: 20px;">'
                f'<h3 style="color: black;">{station_name}</h3>'
                f'<p style="color: brown;">Total Stations: {station_count}</p>'
                f'</div>',
                unsafe_allow_html=True
            )



    st.header(" ")
    st.header(" :blue[Waste-Stations Distribution]")
    tab41, tab42, tab43, tab44 = st.tabs(
    ['Landfills', 'Waste Disposal Centres', 'Waste Transfer Centres', 'Recycling Centres'])

    with tab41:
        # st.subheader("Landfills")
        st.image("images/LFS.png", width=700)
    with tab42:
        # st.subheader("Waste Disposal Centres")
        st.image("images/WDC.png", width=700)
    with tab43:
        # st.subheader("Waste Transfer Centres")
        st.image("images/WTC.png", width=700)
    with tab44:
        # st.subheader("Recycling Centres")
        st.image("images/RC.png", width=700)
    


    st.header(" ")
    st.header(":blue[Heatmap of States Based on Station Information]")
    heatmap_data = station_counts.iloc[:, :-1]
    fig = go.Figure(data=go.Heatmap(
        z=heatmap_data.values,
        x=heatmap_data.columns,
        y=heatmap_data.index,
        colorscale='Viridis',
        colorbar=dict(title='Station Count'),
    ))

    #x-axis stratching
    fig.update_layout(
        autosize=False,
        width=500,  # Adjust the width as needed
        height=500,  # Adjust the height as needed
    )
    st.plotly_chart(fig, use_container_width=True)